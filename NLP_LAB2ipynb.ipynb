{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNQR3GHPga3t",
        "outputId": "cfd2e0a0-b4ba-4803-9c75-b0a959ef0432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def split_sentences(document):\n",
        "    sentences = sent_tokenize(document)\n",
        "    return sentences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example document\n",
        "    document = \"This is a sample document. It has multiple sentences. We want to split it. We will identify the POS tags from the given sentences and words.\"\n",
        "\n",
        "    # Splitting the document into sentences\n",
        "    sentences = split_sentences(document)\n",
        "\n",
        "    # Printing the result\n",
        "    print(\"Original Document:\")\n",
        "    print(document)\n",
        "    print(\"\\nSplit Sentences:\")\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        print(f\"{i}. {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3zT2IKkhr-0",
        "outputId": "9d0165c8-d866-49fa-896b-a2bc5117b1e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Document:\n",
            "This is a sample document. It has multiple sentences. We want to split it. We will identify the POS tags from the given sentences and words.\n",
            "\n",
            "Split Sentences:\n",
            "1. This is a sample document.\n",
            "2. It has multiple sentences.\n",
            "3. We want to split it.\n",
            "4. We will identify the POS tags from the given sentences and words.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Initialize Porter Stemmer\n",
        "    porter_stemmer = PorterStemmer()\n",
        "\n",
        "    # Stem each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "    return words, stemmed_words\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example input string\n",
        "    input_string = \"Tokenization involves breaking text into individual words or tokens, and stemming is the process of reducing words to their base or root form.\"\n",
        "\n",
        "    # Tokenize and stem the input string\n",
        "    original_tokens, stemmed_tokens = tokenize_and_stem(input_string)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"Original Tokens:\")\n",
        "    print(original_tokens)\n",
        "\n",
        "    print(\"\\nStemmed Tokens:\")\n",
        "    print(stemmed_tokens)\n"
      ],
      "metadata": {
        "id": "lG0PO-5bjcqz",
        "outputId": "84fe0a94-2cda-4186-ee2f-c2b11cb2116b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['Tokenization', 'involves', 'breaking', 'text', 'into', 'individual', 'words', 'or', 'tokens', ',', 'and', 'stemming', 'is', 'the', 'process', 'of', 'reducing', 'words', 'to', 'their', 'base', 'or', 'root', 'form', '.']\n",
            "\n",
            "Stemmed Tokens:\n",
            "['token', 'involv', 'break', 'text', 'into', 'individu', 'word', 'or', 'token', ',', 'and', 'stem', 'is', 'the', 'process', 'of', 'reduc', 'word', 'to', 'their', 'base', 'or', 'root', 'form', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_stopwords_and_rare_words(document, stop_words, rare_threshold=1):\n",
        "    # Tokenize the document into words\n",
        "    words = word_tokenize(document.lower())  # Convert to lowercase for case-insensitive matching\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_freq = Counter(filtered_words)\n",
        "\n",
        "    # Remove rare words based on the threshold\n",
        "    filtered_words = [word for word in filtered_words if word_freq[word] > rare_threshold]\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example document\n",
        "    document = \"This is a sample document. It has some stop words and rare words. We want to remove them. WE will perform POS Tagging on sentences and words.\"\n",
        "\n",
        "    # Get NLTK English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove stop words and rare words (words occurring only once)\n",
        "    filtered_words = remove_stopwords_and_rare_words(document, stop_words)\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Original Document:\")\n",
        "    print(document)\n",
        "\n",
        "    print(\"\\nFiltered Words:\")\n",
        "    print(filtered_words)\n"
      ],
      "metadata": {
        "id": "mkyrdJJ9lAsX",
        "outputId": "f234e23b-a264-4400-9fa8-e7de756cac24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Document:\n",
            "This is a sample document. It has some stop words and rare words. We want to remove them. WE will perform POS Tagging on sentences and words.\n",
            "\n",
            "Filtered Words:\n",
            "['.', 'words', 'words', '.', '.', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def identify_parts_of_speech(document):\n",
        "    # Tokenize the document into words\n",
        "    words = word_tokenize(document)\n",
        "\n",
        "    # Perform part-of-speech tagging\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example document\n",
        "    document = \"This is a sample document. It has multiple sentences. We want to identify the parts of speech. We perform POS Tagging on sentences and words.\"\n",
        "\n",
        "    # Identify parts of speech\n",
        "    pos_tags = identify_parts_of_speech(document)\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Original Document:\")\n",
        "    print(document)\n",
        "\n",
        "    print(\"\\nParts of Speech:\")\n",
        "    for word, pos_tag in pos_tags:\n",
        "        print(f\"{word}: {pos_tag}\")\n"
      ],
      "metadata": {
        "id": "3U2uGbXNnfnE",
        "outputId": "dd8a71a2-5c61-4b6b-e834-38c2eeae53bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Document:\n",
            "This is a sample document. It has multiple sentences. We want to identify the parts of speech. We perform POS Tagging on sentences and words.\n",
            "\n",
            "Parts of Speech:\n",
            "This: DT\n",
            "is: VBZ\n",
            "a: DT\n",
            "sample: JJ\n",
            "document: NN\n",
            ".: .\n",
            "It: PRP\n",
            "has: VBZ\n",
            "multiple: JJ\n",
            "sentences: NNS\n",
            ".: .\n",
            "We: PRP\n",
            "want: VBP\n",
            "to: TO\n",
            "identify: VB\n",
            "the: DT\n",
            "parts: NNS\n",
            "of: IN\n",
            "speech: NN\n",
            ".: .\n",
            "We: PRP\n",
            "perform: VBP\n",
            "POS: NNP\n",
            "Tagging: NNP\n",
            "on: IN\n",
            "sentences: NNS\n",
            "and: CC\n",
            "words: NNS\n",
            ".: .\n"
          ]
        }
      ]
    }
  ]
}